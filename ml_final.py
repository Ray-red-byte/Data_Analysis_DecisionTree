# -*- coding: utf-8 -*-
"""ML_FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pX88jw7IZtaM7JrkdCiRstoOsXotrz-r

#<font size=8>Packages
"""

from collections import Counter
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from bs4 import BeautifulSoup
import concurrent.futures
import time
import random
from joblib import Parallel, delayed
import re
from tqdm import tqdm
import nltk
nltk.download('omw-1.4')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences

import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='bs4')

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

from google.colab import drive

drive.mount('/content/gdrive/', force_remount=True)


def read_file(path):
    rawdata = pd.read_csv(path, header=0, delimiter='\t', encoding= 'unicode_escape')
    return rawdata

def preprocess_data(df):
    reviews = []
    for raw in tqdm(df):
        text = BeautifulSoup(raw, 'lxml').get_text()
        only_text = re.sub('[^a-zA-Z]', ' ', text)
        words = word_tokenize(only_text.lower())
        stops = set(stopwords.words('english'))
        non_stopwords = [word for word in words if not word in stops]
        lemma_words = [lemmatizer.lemmatize(word) for word in non_stopwords]
        lemma_words = " ".join(lemma_words)
        reviews.append(lemma_words)
    return reviews

def tokenizer_preprocess(list_X_train, list_X_val):
    unique_words = set()
    len_max = 0
    for sent in tqdm(list_X_train):
        unique_words.update(sent)
        if len_max < len(sent):
            len_max = len(sent)
    len(list(unique_words)), len_max

    tokenizer = Tokenizer(num_words=len(list(unique_words)))
    tokenizer.fit_on_texts(list(list_X_train))                   #Use train_data to fit

    X_train = tokenizer.texts_to_sequences(list_X_train)
    X_train = pad_sequences(X_train, maxlen=len_max)

    X_val = tokenizer.texts_to_sequences(list_X_val)
    X_val = pad_sequences(X_val, maxlen=len_max)

    return X_train, X_val

#cross_validation with k fold
def cross_val(model, X_train, y_train, k=10):
  fold = (X_train.shape[0])/k
  accs = []
  for i in range(k):
    thresh_left, thresh_right = round((i)*fold), round((i+1)*fold)

    if thresh_right < X_train.shape[0] and thresh_left < X_train.shape[0]:
      val_x, val_y = X_train[thresh_left:thresh_right], y_train[thresh_left:thresh_right]
      train_x, train_y = np.concatenate([X_train[:thresh_left], X_train[thresh_right:]]), np.concatenate([y_train[:thresh_left], y_train[thresh_right:]])
    else:
      val_x, val_y = X_train[thresh_left:], y_train[thresh_left:]
      train_x, train_y = X_train[:thresh_left], y_train[:thresh_left]

    model.fit(train_x, train_y)
    prediction = model.predict(val_x)
    acc = accuracy_score(prediction, val_y)
    accs.append(acc)

  return np.mean(accs)

def report(predictions, y_test):
    print('Accuracy: %s' % accuracy_score(y_test, predictions))
    print('Confusion Matrix:')
    print(confusion_matrix(y_test, predictions))
    print('Classification Report:')
    print(classification_report(y_test, predictions))


#Put the Test data
train_x_test = pd.read_csv('./gdrive/MyDrive/Dataset1/X_test_1.csv')

#Put the Train x_data and y_data
train_x_train = pd.read_csv('./gdrive/MyDrive/Dataset1/X_train_1.csv')
train_y_train = pd.read_csv('./gdrive/MyDrive/Dataset1/y_train_1.csv')
train_data = pd.concat([train_x_train, train_y_train], axis=1)


print('------------------------Dataset1----------------------')
train_x_test.describe()
train_data.describe()

"""<font size=5>3. Show missing value before preprocessing\
<font size=3.5>• The all features' missing value ratio are less than 6%, so no need to drop.
"""

train_data.isnull().sum()/len(train_data)

"""<font size=5>4. Drop the duplicated data\
<font size=3.5>• We have to drop the duplicated instances, because they may cause bias.</font>
"""

train_x_test.drop_duplicates(inplace=True)
train_data.drop_duplicates(inplace=True)
train_data.describe()


X_train = train_data.drop("class", axis=1)
y_train = train_data["class"]

X_train.isnull().sum()/X_train.shape[0]


y_train.value_counts(normalize=True)
sns.histplot(data=y_train)


f, axs = plt.subplots(nrows=4, ncols=3, figsize=(18, 15))
for ind, col in zip(axs.flatten()[:-1], X_train.columns):
    sns.histplot(ax=ind, x=col, data=train_data, hue="class", stat="probability", palette="Set2", kde=True)


sns.pairplot(train_data, palette="Set2", hue="class")


sns.heatmap(train_data.corr(), annot=True, fmt='.2f', cmap="Blues")


train_data = train_data.groupby("class").transform(lambda x: x.fillna(x.mean()))
train_data = pd.concat([train_data, y_train], axis=1)
train_data.isnull().sum()/len(train_data)

train_x_test = train_x_test.fillna(train_x_test.mean())

train_data.describe()


f, axs = plt.subplots(nrows=4, ncols=3, figsize=(18, 15))
for ind, col in zip(axs.flatten()[:-1], X_train.columns):
    sns.boxplot(ax=ind, x=X_train[col])


for col in train_data.columns:
  if col == 'class':
    continue
  n_std = 1.5
  mean = train_data[col].mean()
  sd = train_data[col].std()

  df = train_data[(train_data[col] <= mean+(n_std*sd)) & (train_data[col] > mean-(n_std*sd))]

train_data = df
train_data.describe()

train_data.to_csv('./gdrive/MyDrive/Dataset1/processed_train_dataset1.csv')
train_x_test.to_csv('./gdrive/MyDrive/Dataset1/processed_test_dataset1.csv')


#Put the Test data
df_phrase_test = read_file('./gdrive/MyDrive/Dataset2/X_test_2.csv')

#Put the Train x_data and y_data
df_phrase_train = read_file('./gdrive/MyDrive/Dataset2/X_train_2.csv')
df_sentimental_train = read_file('./gdrive/MyDrive/Dataset2/y_train_2.csv')

len_train, len_test = df_phrase_train.shape[0], df_phrase_test.shape[0]
X, y = pd.concat([df_phrase_train['Phrase'], df_phrase_test['Phrase']], axis=0), df_sentimental_train['Sentiment']


print('------------------------Dataset2----------------------')
print(df_phrase_train.describe())
print(df_phrase_test.describe())
print(df_sentimental_train.describe())


#Distribution of length of each class
df_phrase_train['PhraseLength']=df_phrase_train['Phrase'].apply(lambda x: len(x))
df_phrase_train.sort_values(by='PhraseLength', ascending=False).head()

plt.figure(figsize=(16,7))
bins=np.linspace(0,200,50)
plt.hist(df_phrase_train[df_sentimental_train['Sentiment']==0]['PhraseLength'],bins=bins,density=True,label='negative')
plt.hist(df_phrase_train[df_sentimental_train['Sentiment']==1]['PhraseLength'],bins=bins,density=True,label='somewhat negative')
plt.hist(df_phrase_train[df_sentimental_train['Sentiment']==2]['PhraseLength'],bins=bins,density=True,label='neutral')
plt.hist(df_phrase_train[df_sentimental_train['Sentiment']==3]['PhraseLength'],bins=bins,density=True,label='somewhat positive')
plt.hist(df_phrase_train[df_sentimental_train['Sentiment']==4]['PhraseLength'],bins=bins,density=True,label='positive')
plt.xlabel('Phrase length')
plt.legend()
plt.show()

plt.hist(y, bins=100)
plt.show()

train_x_text = preprocess_data(X)


#Select top 1000 features in Tfidf, since the scarcely occur word will be eliminated
TF = TfidfVectorizer(max_features=1000)
TF_data = TF.fit_transform(train_x_text)

#Split the concate_data into original train and test
TF_train, TF_test = TF_data[:len_train], TF_data[len_train:]


class Node:
    '''
        The information of node
        1. The location of feature
        2. The threshold to partition
        3. if reach leaf_node then return value
    '''
    def __init__(self, feat_loc=None, threshold=None, left=None, right=None, value=None, depth=None):
        self.feat_loc = feat_loc
        self.threshold = threshold
        self.left = left
        self.right = right
        self.value = value
        self.depth = depth

    def leaf(self):
        if self.value is not None:
            return self.value

class Decision_Tree:

    def __init__(self, max_depth=None, min_samples=None, feat=None, n_job=-1, random_n_feat=10, select_function="entropy"):  #傳入的Data設定DT的邊界條件
        '''
            The hyperparameter of DT
            1. max_depth of the tree
            2. min_sample of each node
            3. numbers of features
            4. n_job of the cores of paralleling select best_feature and best_threshold
            5. random_n_feat of the original features of Data
            5. select_function is the criterion to calculate Parent and Child node
        '''
        self.max_depth = max_depth
        self.min_samples = min_samples
        self.feat = feat
        self.n_job = n_job
        self.random_n_feat = random_n_feat
        self.tree = None
        self.evaluation = None

        if select_function == 'entropy':
            self.evaluation = self.entropy
        if select_function == 'Gini':
            self.evaluation = self.Gini

    #Entropy
    def entropy(self, y):
        y = y.flatten()
        unique, counts = np.unique(y, return_counts=True)
        if len(unique) == 0:
            return 0
        total = np.sum(counts)
        return -np.sum(np.log2(counts/total))
        # return -np.sum([((j/total)*np.log2(j/total)) for j in counts if j > 0])

    #Gini
    def Gini(self, y):
        y = y.flatten()
        unique, counts = np.unique(y, return_counts=True)
        total = np.sum(counts)
        return 1 - np.sum([(i/total) for i in counts])

    def fit(self, X, y):
        #Random select the number of feature to be selected
        n_features = self.feat

        self.tree = self._grow_tree(X, y, n_features)

    def predict(self, test_x):
        all_prediction = []

        #Iteratively get each instance in test_x
        for i in test_x:

            #Because the data type I use include sparse metrix and ndarray.
            if isinstance(i, np.ndarray):
              x = i.flatten()
            else:
              x = i.toarray().flatten()

            prediction = self._classify(x, self.tree)
            all_prediction.append(prediction)

        return np.array(all_prediction)

    def _classify(self, each_test_data, node):

        #Stopping Criterion, if the test instance reach the leaf_node
        if node.leaf() is not None:
            return node.value

        #If the test instance is smaller than node threshold, go to left, vice versa.
        if each_test_data[node.feat_loc] <= node.threshold:
            return self._classify(each_test_data, node.left)

        return self._classify(each_test_data, node.right)

    def _grow_tree(self, x, y, n_features, depth = 0):

        #Stopping Criterion
        unique, counts = np.unique(y, return_counts=True)
        if (len(y) <= self.min_samples) or (depth > self.max_depth) or (len(counts) == 1):
          leaf_value = self._leaf_value(y)
          return Node(value=leaf_value, depth=depth)

        #Find the best feature and threshold based on largest information gain
        selected_feature, selected_threshold = self._selected_feature_threshold(x, y, n_features)

        #Split the data based on the best feature and threshold I find
        right, left = self._split(x, selected_feature, selected_threshold)

        #Somtimes, there's the probability that the partition cause to empty node
        if right.shape[0] == 0 or left.shape[0] == 0:
          leaf_value = self._leaf_value(y)
          return Node(value=leaf_value, depth=depth)

        left_x, left_y = x[left, :], y[left]
        right_x, right_y = x[right, :], y[right]

        #After classify the instance, go to the lower node
        l = self._grow_tree(left_x, left_y, n_features, depth + 1)
        r = self._grow_tree(right_x, right_y, n_features, depth + 1)

        return Node(selected_feature, selected_threshold, l, r, depth=depth)

    def _leaf_value(self, y):

        #Calculate the value of the node, based on the most frequency value
        y = y.flatten()
        counter = Counter(y)
        value = counter.most_common(1)[0][0]
        return value

    def _split(self, x, column, split_point):

        #Split the instances, set the split_point as threshold to separate.
        left = np.argwhere(x[:, column] <= split_point)
        right = np.argwhere(x[:, column] > split_point)

        return right[:, 0], left[:, 0]

    def _selected_feature_threshold(self, x, y, n_features):
        target = y

        #Random select feature in each node
        n_features = random.sample(n_features, self.random_n_feat)

        #Compute the largest IG(information gain) among all instances in each column of feature
        def compute_ig(column, target):
          best_IG, best_feature, best_split_point = 0, -1, -1

          if isinstance(x[:, column], np.ndarray):
            nonzero_rows = np.unique(x[:, column]).flatten()
            min_value = np.mean(x[:, column])
          else:
            nonzero_rows = np.unique(x[:, column].toarray()).flatten()
            min_value = 0


          for split_point in nonzero_rows:
            if split_point == min_value or split_point == 0:
              continue

            #Use the target which is before splitting
            parent_entropy = self.evaluation(target)

            #Use the x_data and the selected split_point and selected feature to split
            right_id, left_id = self._split(x, column, split_point)
            left, right = y[left_id], y[right_id]
            IG = self._information_gain(parent_entropy, left, right)

            #if the IG in the selected threshold higher than previous largest one,
            #Change 1.best_IG, 2.best_feature, 3.best_split_point
            if abs(IG) > best_IG:
              best_IG = abs(IG)
              best_feature = column
              best_split_point = split_point

          return best_IG, best_feature, best_split_point

        #The result save all best IG, feature, threshold among all  features.
        #Use the n_jobs CPU_CORES to accelerate the code
        result = Parallel(n_jobs=self.n_job, verbose=20)(delayed(compute_ig)(column, target=target) for column in n_features)

        #Select the best IG, feature, threshold in best feature, according to IG
        max_IG_id = np.argmax(result, axis=0)
        best_feature, best_split_point = result[max_IG_id[0]][1], result[max_IG_id[0]][2]

        return best_feature, best_split_point

    def _information_gain(self, parent_entropy, left, right):
        #Information of Parent minus Information of Child
        portion_l = len(left)/(len(left) + len(right))
        portion_r = len(right)/(len(left) + len(right))

        #Evalution depends on the user's input.
        child_entropy = portion_l*self.evaluation(left) + portion_r*self.evaluation(right)
        IG = parent_entropy - child_entropy
        return IG


#Split the train into train_x and train_y(target)
train = train_data.to_numpy()
x_train, y_train = train[:, :11], train[:, -1]

#Prepare X_test_1
X_test_1 = train_x_test.to_numpy()

#Split the train_x and train_y into train_x, test_x, train_y, test_y, stratify according to train_y, because of imbalanced class
X_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(x_train, y_train, test_size=0.2, stratify=y_train)

#Prepare X_test_2
X_test_2 = TF_test

#Split the TF_train into train and test
X_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(TF_train, y, test_size=0.2, stratify=y)
y_train_2, y_val_2 = y_train_2.to_numpy(), y_val_2.to_numpy()


#Use entropy criteria
DT_entropy_1 = Decision_Tree(max_depth=10, min_samples=10, random_n_feat=11, feat=range(X_train_1.shape[1]), select_function='entropy')
DT_entropy_1.fit(X_train_1, y_train_1)

#Use Gini criteria
DT_Gini_1 = Decision_Tree(max_depth=10, min_samples=10, random_n_feat=11, feat=range(X_train_1.shape[1]), select_function='Gini')
DT_Gini_1.fit(X_train_1, y_train_1)


#Use entropy criteria
DT_entropy_2 = Decision_Tree(max_depth=10, min_samples=10, random_n_feat=100, feat=range(X_train_2.shape[1]), select_function='entropy')
DT_entropy_2.fit(X_train_2, y_train_2)

#Use Gini criteria
DT_Gini_2 = Decision_Tree(max_depth=10, min_samples=10, random_n_feat=100, feat=range(X_train_2.shape[1]), select_function='Gini')
DT_Gini_2.fit(X_train_2, y_train_2)


Prediction_entropy_1 = DT_entropy_1.predict(X_val_1)
Prediction_Gini_1 = DT_Gini_1.predict(X_val_1)

print('--------------------------Entropy Dataset1-----------------------')
report(Prediction_entropy_1, y_val_1)
print(Prediction_entropy_1)
print('----------------------------Gini Dataset1-----------------------')
report(Prediction_Gini_1, y_val_1)
print(Prediction_Gini_1)

x = Prediction_entropy_1
pd.DataFrame(x).to_csv('./gdrive/MyDrive/Dataset1/dataset1_prediction.csv')


Prediction_entropy_test = DT_entropy_1.predict(X_test_1)
print('--------------------------Entropy Dataset1 Test-----------------------')
# report(Prediction_entrop_test, 'Test target')
print(Prediction_entropy_test)

Prediction_Gini_test = DT_Gini_1.predict(X_test_1)
print('----------------------------Gini Dataset1 Test-----------------------')
# report(Prediction_Gini_test, 'Test target')
print(Prediction_Gini_test)


Prediction_entropy_2 = DT_entropy_2.predict(X_val_2)
Prediction_Gini_2 = DT_Gini_2.predict(X_val_2)

print('--------------------------Entropy Dataset2 Validation-----------------------')
report(Prediction_entropy_2, y_val_2)
print(Prediction_entropy_2
      )
print('----------------------------Gini Dataset2 Validation-----------------------')
report(Prediction_Gini_2, y_val_2)
print(Prediction_Gini_2)


Prediction_entropy_test_2 = DT_entropy_2.predict(X_test_2)
print('--------------------------Entropy Dataset2 Test-----------------------')
# report(Prediction_entrop_test_2, 'Test target')
print(Prediction_entropy_test_2)

Prediction_Gini_test_2 = DT_Gini_2.predict(X_test_2)
print('----------------------------Gini Dataset2 Test-----------------------')
# report(Prediction_Gini_test_2, 'Test target')
print(Prediction_Gini_test_2)